{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## usar python 3.8.8\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando SparkSession para criar uma sessão do Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importando funções e tipos de dados SparkSQL\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Importando módulos Spark MLlib\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import Imputer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "\n",
    "# Importando SparkContext e SparkConf\n",
    "from pyspark import SparkContext, SparkConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Criando uma nova sessão do Spark\n",
    "# Spark entry point\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"modeling-pkdd99-xpeMBA\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_df_csv(tabela=str):\n",
    "    \"\"\"\n",
    "    Função para as bases de dados onde retorna no print o 'shape', um breve 'show' e o Scheema das variáveis.\n",
    "    :param entidade_name: string que referencie o nome da tabela que complete o caminho './dados_originais/{tabela}.csv'. \n",
    "    tabela pode ser => trans, account, card, client, disp, district, loan, order \n",
    "    :return: DataFrame em pyspark\n",
    "    \"\"\"\n",
    "    path =\"C:\\\\Users\\\\renat\\\\Documents\\\\00_MBA\\\\PROJETO_APLICADO\\\\ML-predict-loan-MBA-applied-project\\\\dados_modelagem\"\n",
    "    df = spark.read.csv(path = f'{path}/{tabela}.csv', header='True',inferSchema='True', sep=',')\n",
    "    print('\\n','A base de dados possui:',df.count(), 'linhas', 'e', len(df.columns), 'colunas', '\\n')\n",
    "    print(df.show(5))\n",
    "    print(df.printSchema())\n",
    "    return(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " A base de dados possui: 827 linhas e 26 colunas \n",
      "\n",
      "+----------+-------+---------+---------------+----------------+--------+----------+---------+-------+----------+------+--------+--------+------+----------+------------------+-------+---------+--------+-------+-----------------+------------------+--------+--------+-----+------------+\n",
      "|account_id|disp_id|client_id|account_id_acct|district_id_bank|stmt_frq| date_acct|type_disp|loan_id| date_loan|amount|duration|payments|status|date_birth|district_id_client|card_id|type_card|    min1|   max1|            mean1|             mean6|response|has_card|idade|days_between|\n",
      "+----------+-------+---------+---------------+----------------+--------+----------+---------+-------+----------+------+--------+--------+------+----------+------------------+-------+---------+--------+-------+-----------------+------------------+--------+--------+-----+------------+\n",
      "|     10351|  12430|    12738|          10351|              23| monthly|1995-05-04|    owner|   7115|1997-03-04| 88704|      48|  1848.0|     C|1960-10-29|                23|   null|     null| 11853.6| 9953.6|       18891.7448| 19977.38913043479|       1|       0|   36|         670|\n",
      "|     10351|  12431|    12739|          10351|              23| monthly|1995-05-04|disponent|   7115|1997-03-04| 88704|      48|  1848.0|     C|1958-01-17|                23|   null|     null| 11853.6| 9953.6|       18891.7448| 19977.38913043479|       1|       0|   39|         670|\n",
      "|     10436|  12532|    12840|          10436|              60| monthly|1996-03-21|    owner|   7136|1996-09-28| 54396|      36|  1511.0|     C|1939-07-27|                60|   null|     null| 18196.0| 8296.0|      27385.44375|             800.0|       1|       0|   57|         191|\n",
      "|      4937|   5965|     5965|           4937|              12| monthly|1993-02-20|    owner|   6004|1994-07-23|143904|      24|  5996.0|     A|1951-04-10|                50|   null|     null|100282.7|96774.9|49212.63181818183|        49524.9625|       1|       0|   43|         518|\n",
      "|       675|    810|      810|            675|              58| monthly|1995-08-03|    owner|   5110|1997-04-01|102240|      60|  1704.0|     C|1945-09-24|                58|  142.0|  classic| 10438.0|  900.0|29672.60504201679|28522.888607594923|       1|       1|   51|         607|\n",
      "+----------+-------+---------+---------------+----------------+--------+----------+---------+-------+----------+------+--------+--------+------+----------+------------------+-------+---------+--------+-------+-----------------+------------------+--------+--------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- account_id: integer (nullable = true)\n",
      " |-- disp_id: integer (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- account_id_acct: integer (nullable = true)\n",
      " |-- district_id_bank: integer (nullable = true)\n",
      " |-- stmt_frq: string (nullable = true)\n",
      " |-- date_acct: string (nullable = true)\n",
      " |-- type_disp: string (nullable = true)\n",
      " |-- loan_id: integer (nullable = true)\n",
      " |-- date_loan: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- payments: double (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- date_birth: string (nullable = true)\n",
      " |-- district_id_client: integer (nullable = true)\n",
      " |-- card_id: double (nullable = true)\n",
      " |-- type_card: string (nullable = true)\n",
      " |-- min1: double (nullable = true)\n",
      " |-- max1: double (nullable = true)\n",
      " |-- mean1: double (nullable = true)\n",
      " |-- mean6: double (nullable = true)\n",
      " |-- response: integer (nullable = true)\n",
      " |-- has_card: integer (nullable = true)\n",
      " |-- idade: integer (nullable = true)\n",
      " |-- days_between: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_model = read_df_csv('df_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(account_id=10351, disp_id=12430, client_id=12738, account_id_acct=10351, district_id_bank=23, stmt_frq='monthly', date_acct='1995-05-04', type_disp='owner', loan_id=7115, date_loan='1997-03-04', amount=88704, duration=48, payments=1848.0, status='C', date_birth='1960-10-29', district_id_client=23, card_id=None, type_card=None, min1=11853.6, max1=9953.6, mean1=18891.7448, mean6=19977.38913043479, response=1, has_card=0, idade=36, days_between=670)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.take(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo as variáveis em explicativas e resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount',\n",
       " 'duration',\n",
       " 'payments',\n",
       " 'min1',\n",
       " 'max1',\n",
       " 'mean1',\n",
       " 'mean6',\n",
       " 'has_card',\n",
       " 'idade',\n",
       " 'days_between']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model2 = df_model.select('amount', 'duration', 'payments', 'min1', 'max1', 'mean1', 'mean6', 'has_card', 'idade', 'days_between', 'response')\n",
    "\n",
    "df_model2 =  df_model2.withColumnRenamed('response', 'label')\n",
    "\n",
    "cols = df_model2.columns[:-1]\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- payments: double (nullable = true)\n",
      " |-- min1: double (nullable = true)\n",
      " |-- max1: double (nullable = true)\n",
      " |-- mean1: double (nullable = true)\n",
      " |-- mean6: double (nullable = true)\n",
      " |-- has_card: integer (nullable = true)\n",
      " |-- idade: integer (nullable = true)\n",
      " |-- days_between: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+----+----+-----+-----+--------+-----+------------+-----+\n",
      "|amount|duration|payments|min1|max1|mean1|mean6|has_card|idade|days_between|label|\n",
      "+------+--------+--------+----+----+-----+-----+--------+-----+------------+-----+\n",
      "|     0|       0|       0|   0|   0|    0|   98|       0|    0|           0|    0|\n",
      "+------+--------+--------+----+----+-----+-----+--------+-----+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# contar valores ausentes por coluna\n",
    "missing_counts = df_model2.select([f.sum(f.col(c).isNull().cast(\"int\")).alias(c) for c in df_model2.columns])\n",
    "\n",
    "# mostrar o resultado\n",
    "missing_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount',\n",
       " 'duration',\n",
       " 'payments',\n",
       " 'min1',\n",
       " 'max1',\n",
       " 'mean1',\n",
       " 'mean6',\n",
       " 'has_card',\n",
       " 'idade',\n",
       " 'days_between',\n",
       " 'label']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|            mean6|\n",
      "+-------+-----------------+\n",
      "|   mean|40452.95037533935|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model2.select('mean6').summary('mean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model2 = df_model2.fillna({'mean6': 40452.95})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar um VectorAssembler para transformar as variáveis explicativas em uma única coluna de vetores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o VectorAssembler para unir as colunas em uma única coluna vetorizada\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=df_model2.columns[:-1],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MinMaxScaler ou o StandardScaler para dimensionar os dados para que características como salário, idade e renda contribuam igualmente para a análise contribuam igualmente para a análise. Ao dimensionar os dados, podemos garantir que cada recurso tenha um impacto igual no desempenho do modelo, e o modelo pode fazer previsões mais precisas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conceito de normalização é implementado em Python usando MinMaxScaler e o conceito de padronização é implementado usando StandardScaler.\n",
    " MinMaxScaler dimensiona os dados para um intervalo fixo, normalmente entre 0 e 1. Por outro lado, StandardScaler redimensiona os dados para que tenham uma média de 0 e um desvio padrão de 1. Isso resulta em uma distribuição com média zero e variação de unidade. \n",
    " \n",
    " https://vitalflux.com/minmaxscaler-standardscaler-python-examples/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o StandardScaler para normalizar as variáveis\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaledFeatures\",\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o pipeline para unir as etapas de pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o Pipeline com as etapas de pré-processamento e o modelo\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(df_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = pipelineModel.transform(df_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+--------+-------+------------------+------------------+--------+-----+------------+-----+--------------------+--------------------+\n",
      "|amount|duration|payments|    min1|   max1|             mean1|             mean6|has_card|idade|days_between|label|            features|      scaledFeatures|\n",
      "+------+--------+--------+--------+-------+------------------+------------------+--------+-----+------------+-----+--------------------+--------------------+\n",
      "| 88704|      48|  1848.0| 11853.6| 9953.6|        18891.7448| 19977.38913043479|       0|   36|         670|    1|[88704.0,48.0,184...|[0.77418650865161...|\n",
      "| 88704|      48|  1848.0| 11853.6| 9953.6|        18891.7448| 19977.38913043479|       0|   39|         670|    1|[88704.0,48.0,184...|[0.77418650865161...|\n",
      "| 54396|      36|  1511.0| 18196.0| 8296.0|       27385.44375|             800.0|       0|   57|         191|    1|[54396.0,36.0,151...|[0.47475479487524...|\n",
      "|143904|      24|  5996.0|100282.7|96774.9| 49212.63181818183|        49524.9625|       0|   43|         518|    1|[143904.0,24.0,59...|[1.25595841609174...|\n",
      "|102240|      60|  1704.0| 10438.0|  900.0| 29672.60504201679|28522.888607594923|       1|   51|         607|    1|[102240.0,60.0,17...|[0.89232535899780...|\n",
      "|102240|      60|  1704.0| 10438.0|  900.0| 29672.60504201679|28522.888607594923|       0|   56|         607|    1|[102240.0,60.0,17...|[0.89232535899780...|\n",
      "|155616|      48|  3242.0| 17792.0|73261.2| 43766.96909090906|  44716.1087301587|       0|   25|         691|    1|[155616.0,48.0,32...|[1.35817784688774...|\n",
      "|155616|      48|  3242.0| 17792.0|73261.2| 43766.96909090906|  44716.1087301587|       0|   17|         691|    1|[155616.0,48.0,32...|[1.35817784688774...|\n",
      "|104712|      24|  4363.0|116809.2|97532.7| 52700.12647058825| 41260.86034482761|       0|   25|         569|    1|[104712.0,24.0,43...|[0.91390036180925...|\n",
      "|104712|      24|  4363.0|116809.2|97532.7| 52700.12647058825| 41260.86034482761|       0|   18|         569|    1|[104712.0,24.0,43...|[0.91390036180925...|\n",
      "| 56100|      60|   935.0|  1000.0| 9899.1|20549.230158730166|26157.734482758613|       0|   37|         405|    1|[56100.0,60.0,935...|[0.48962688419187...|\n",
      "|385584|      48|  8033.0|106530.1|98638.1| 71744.03055555554| 60002.15294117648|       0|   23|         502|    1|[385584.0,48.0,80...|[3.36528150649266...|\n",
      "|385584|      48|  8033.0|106530.1|98638.1| 71744.03055555554| 60002.15294117648|       0|   19|         502|    1|[385584.0,48.0,80...|[3.36528150649266...|\n",
      "| 69624|      36|  1934.0|  1100.0|74473.4|48848.771223021584| 48824.17916666667|       1|   51|         585|    1|[69624.0,36.0,193...|[0.60766100151470...|\n",
      "|100800|      24|  4200.0| -1033.8|92354.7| 28517.63666666666| 42296.14230769231|       0|   59|         469|    0|[100800.0,24.0,42...|[0.87975739619502...|\n",
      "|399120|      60|  6652.0| 12000.0|85656.2| 43888.80178571428| 48992.67142857142|       0|   54|         450|    1|[399120.0,60.0,66...|[3.48342035683885...|\n",
      "| 12540|      12|  1045.0| 14858.0|57779.4| 41539.73636363639| 38930.43495145633|       1|   57|         623|    1|[12540.0,12.0,104...|[0.10944600940759...|\n",
      "| 12540|      12|  1045.0| 14858.0|57779.4| 41539.73636363639| 38930.43495145633|       0|   61|         623|    1|[12540.0,12.0,104...|[0.10944600940759...|\n",
      "| 30276|      12|  2523.0|  1000.0| 8322.5| 31481.05952380953| 34767.38666666667|       0|   57|         388|    0|[30276.0,12.0,252...|[0.26424141792857...|\n",
      "|191088|      48|  3981.0| 12325.4|57550.1|32398.580555555556| 37199.23684210526|       0|   34|         408|    1|[191088.0,48.0,39...|[1.66776866392970...|\n",
      "+------+--------+--------+--------+-------+------------------+------------------+--------+-----+------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide o dataset em conjuntos de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainData, testData) = df_transformed.randomSplit([0.7, 0.3], seed=12345)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o modelo de Regressão Logística"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No MLLib, a Regressão Logística não possui um método de cálculo de feature importance embutido. No entanto, é possível calcular a importância das features de forma aproximada usando o método \"L1 Regularization\" (também conhecido como \"Lasso regularization\").\n",
    "\n",
    "Essa técnica de regularização penaliza os coeficientes das features que não são relevantes para a predição, forçando-os a terem valor zero. Dessa forma, as features com coeficientes não-nulos são consideradas mais importantes.\n",
    "\n",
    "Para realizar esse método no MLLib, basta usar o parâmetro \"elasticNetParam\" da função LogisticRegression, que controla a proporção de regularização L1 e L2. Ao definir o valor de \"elasticNetParam\" como 1 (que significa 100% de regularização L1), a regressão logística irá utilizar apenas L1 regularization, o que resultará em alguns coeficientes com valor zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'scaledFeatures', labelCol = 'label', elasticNetParam=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define os parâmetros a serem testados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#paramGrid = ParamGridBuilder() \\\n",
    "#    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "#    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "#    .build()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o avaliador para a validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol='label',\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo a validação cruzada com 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \n",
    "#crossval = CrossValidator(\n",
    "#    estimator=lr,\n",
    "#    estimatorParamMaps=paramGrid,\n",
    "#    evaluator=evaluator,\n",
    "#    numFolds=5\n",
    "#)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajusta o modelo com a validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#modelo = crossval.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "modelo = lr.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = modelo.coefficients.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.44140367,  0.19925389, -0.19457131,  0.46574029, -0.36123858,\n",
       "        0.67418071, -0.28387973,  0.39524176, -0.11496371,  0.31963519])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtém as features mais importantes (aquelas com coeficiente não nulo)\n",
    "important_features = [i for i, coef in enumerate(coefficients) if coef != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = list(zip(assembler.getInputCols(), modelo.coefficients.toArray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amount', -0.44140366598496533),\n",
       " ('duration', 0.1992538902862431),\n",
       " ('payments', -0.1945713091696159),\n",
       " ('min1', 0.46574029122558336),\n",
       " ('max1', -0.36123857645703733),\n",
       " ('mean1', 0.6741807087636067),\n",
       " ('mean6', -0.2838797275156386),\n",
       " ('has_card', 0.3952417574842975),\n",
       " ('idade', -0.11496370821418399),\n",
       " ('days_between', 0.31963519439202237)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on training data = 0.752386\n"
     ]
    }
   ],
   "source": [
    "# assuming `model` is your trained LogisticRegressionModel object, and `trainData` is your training data DataFrame\n",
    "predictions_train = modelo.transform(trainData)\n",
    "auc_train = evaluator.evaluate(predictions_train)\n",
    "print(\"AUC on training data = %g\" % auc_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avalia o modelo com os dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on training data = 0.734202\n"
     ]
    }
   ],
   "source": [
    "# assuming `model` is your trained LogisticRegressionModel object, and `trainData` is your training data DataFrame\n",
    "predictions_teste = modelo.transform(testData)\n",
    "auc_test = evaluator.evaluate(predictions_teste)\n",
    "print(\"AUC on training data = %g\" % auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- payments: double (nullable = true)\n",
      " |-- min1: double (nullable = true)\n",
      " |-- max1: double (nullable = true)\n",
      " |-- mean1: double (nullable = true)\n",
      " |-- mean6: double (nullable = false)\n",
      " |-- has_card: integer (nullable = true)\n",
      " |-- idade: integer (nullable = true)\n",
      " |-- days_between: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_teste.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|probability                              |\n",
      "+-----------------------------------------+\n",
      "|[0.057585893661319866,0.9424141063386803]|\n",
      "|[0.024207806939383646,0.9757921930606164]|\n",
      "|[0.017197850174292668,0.9828021498257073]|\n",
      "|[0.038465008801542966,0.961534991198457] |\n",
      "|[0.07434340271245267,0.9256565972875472] |\n",
      "|[0.04062857484270776,0.9593714251572923] |\n",
      "|[0.07861198400648306,0.9213880159935169] |\n",
      "|[0.0689411472811062,0.9310588527188939]  |\n",
      "|[0.09163813490888457,0.9083618650911155] |\n",
      "|[0.09927361069329263,0.9007263893067073] |\n",
      "|[0.03748266207665693,0.9625173379233432] |\n",
      "|[0.04463708220051664,0.9553629177994833] |\n",
      "|[0.0584679374786745,0.9415320625213255]  |\n",
      "|[0.016960692927537,0.983039307072463]    |\n",
      "|[0.03546721660805609,0.9645327833919438] |\n",
      "|[0.04016570078116782,0.9598342992188322] |\n",
      "|[0.011957337745994341,0.9880426622540057]|\n",
      "|[0.0411236486029402,0.9588763513970598]  |\n",
      "|[0.04688460051171686,0.9531153994882832] |\n",
      "|[0.13646218129213358,0.8635378187078664] |\n",
      "+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_teste.select('probability').show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando as métricas do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.949816\n",
      "Recall = 0.734202\n"
     ]
    }
   ],
   "source": [
    "# Importa as métricas de avaliação\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Cria o avaliador com a métrica de área sob a curva PR (precisão e recall)\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderPR\")\n",
    "\n",
    "# Usa o modelo para fazer previsões na base de teste\n",
    "predictions_teste = modelo.transform(testData)\n",
    "\n",
    "# Calcula a métrica de precisão\n",
    "precision = evaluator.evaluate(predictions_teste)\n",
    "\n",
    "# Calcula a métrica de recall\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "recall = evaluator.evaluate(predictions_teste)\n",
    "\n",
    "\n",
    "print(\"Precision = %g\" % precision)\n",
    "print(\"Recall = %g\" % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
