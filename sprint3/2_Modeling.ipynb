{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## usar python 3.8.8\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando SparkSession para criar uma sessão do Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importando funções e tipos de dados SparkSQL\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Importando módulos Spark MLlib\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import Imputer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "\n",
    "# Importando SparkContext e SparkConf\n",
    "from pyspark import SparkContext, SparkConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Criando uma nova sessão do Spark\n",
    "# Spark entry point\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"modeling-pkdd99-xpeMBA\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_df_csv(tabela=str):\n",
    "    \"\"\"\n",
    "    Função para as bases de dados onde retorna no print o 'shape', um breve 'show' e o Scheema das variáveis.\n",
    "    :param entidade_name: string que referencie o nome da tabela que complete o caminho './dados_originais/{tabela}.csv'. \n",
    "    tabela pode ser => trans, account, card, client, disp, district, loan, order \n",
    "    :return: DataFrame em pyspark\n",
    "    \"\"\"\n",
    "    path =\"C:\\\\Users\\\\renat\\\\Documents\\\\00_MBA\\\\PROJETO_APLICADO\\\\ML-predict-loan-MBA-applied-project\\\\dados_modelagem\"\n",
    "    df = spark.read.csv(path = f'{path}/{tabela}.csv', header='True',inferSchema='True', sep=',')\n",
    "    print('\\n','A base de dados possui:',df.count(), 'linhas', 'e', len(df.columns), 'colunas', '\\n')\n",
    "    print(df.show(5))\n",
    "    print(df.printSchema())\n",
    "    return(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " A base de dados possui: 827 linhas e 26 colunas \n",
      "\n",
      "+----------+-------+---------+---------------+----------------+--------+----------+---------+-------+----------+------+--------+--------+------+----------+------------------+-------+---------+--------+-------+-----------------+------------------+--------+--------+-----+------------+\n",
      "|account_id|disp_id|client_id|account_id_acct|district_id_bank|stmt_frq| date_acct|type_disp|loan_id| date_loan|amount|duration|payments|status|date_birth|district_id_client|card_id|type_card|    min1|   max1|            mean1|             mean6|response|has_card|idade|days_between|\n",
      "+----------+-------+---------+---------------+----------------+--------+----------+---------+-------+----------+------+--------+--------+------+----------+------------------+-------+---------+--------+-------+-----------------+------------------+--------+--------+-----+------------+\n",
      "|     10351|  12430|    12738|          10351|              23| monthly|1995-05-04|    owner|   7115|1997-03-04| 88704|      48|  1848.0|     C|1960-10-29|                23|   null|     null| 11853.6| 9953.6|       18891.7448| 19977.38913043479|       1|       0|   36|         670|\n",
      "|     10351|  12431|    12739|          10351|              23| monthly|1995-05-04|disponent|   7115|1997-03-04| 88704|      48|  1848.0|     C|1958-01-17|                23|   null|     null| 11853.6| 9953.6|       18891.7448| 19977.38913043479|       1|       0|   39|         670|\n",
      "|     10436|  12532|    12840|          10436|              60| monthly|1996-03-21|    owner|   7136|1996-09-28| 54396|      36|  1511.0|     C|1939-07-27|                60|   null|     null| 18196.0| 8296.0|      27385.44375|             800.0|       1|       0|   57|         191|\n",
      "|      4937|   5965|     5965|           4937|              12| monthly|1993-02-20|    owner|   6004|1994-07-23|143904|      24|  5996.0|     A|1951-04-10|                50|   null|     null|100282.7|96774.9|49212.63181818183|        49524.9625|       1|       0|   43|         518|\n",
      "|       675|    810|      810|            675|              58| monthly|1995-08-03|    owner|   5110|1997-04-01|102240|      60|  1704.0|     C|1945-09-24|                58|  142.0|  classic| 10438.0|  900.0|29672.60504201679|28522.888607594923|       1|       1|   51|         607|\n",
      "+----------+-------+---------+---------------+----------------+--------+----------+---------+-------+----------+------+--------+--------+------+----------+------------------+-------+---------+--------+-------+-----------------+------------------+--------+--------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- account_id: integer (nullable = true)\n",
      " |-- disp_id: integer (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- account_id_acct: integer (nullable = true)\n",
      " |-- district_id_bank: integer (nullable = true)\n",
      " |-- stmt_frq: string (nullable = true)\n",
      " |-- date_acct: string (nullable = true)\n",
      " |-- type_disp: string (nullable = true)\n",
      " |-- loan_id: integer (nullable = true)\n",
      " |-- date_loan: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- payments: double (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- date_birth: string (nullable = true)\n",
      " |-- district_id_client: integer (nullable = true)\n",
      " |-- card_id: double (nullable = true)\n",
      " |-- type_card: string (nullable = true)\n",
      " |-- min1: double (nullable = true)\n",
      " |-- max1: double (nullable = true)\n",
      " |-- mean1: double (nullable = true)\n",
      " |-- mean6: double (nullable = true)\n",
      " |-- response: integer (nullable = true)\n",
      " |-- has_card: integer (nullable = true)\n",
      " |-- idade: integer (nullable = true)\n",
      " |-- days_between: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_model = read_df_csv('df_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(account_id=10351, disp_id=12430, client_id=12738, account_id_acct=10351, district_id_bank=23, stmt_frq='monthly', date_acct='1995-05-04', type_disp='owner', loan_id=7115, date_loan='1997-03-04', amount=88704, duration=48, payments=1848.0, status='C', date_birth='1960-10-29', district_id_client=23, card_id=None, type_card=None, min1=11853.6, max1=9953.6, mean1=18891.7448, mean6=19977.38913043479, response=1, has_card=0, idade=36, days_between=670)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.take(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo as variáveis em explicativas e resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount',\n",
       " 'duration',\n",
       " 'payments',\n",
       " 'min1',\n",
       " 'max1',\n",
       " 'mean1',\n",
       " 'mean6',\n",
       " 'has_card',\n",
       " 'idade',\n",
       " 'days_between']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model2 = df_model.select('amount', 'duration', 'payments', 'min1', 'max1', 'mean1', 'mean6', 'has_card', 'idade', 'days_between', 'response')\n",
    "\n",
    "df_model2 =  df_model2.withColumnRenamed('response', 'label')\n",
    "\n",
    "cols = df_model2.columns[:-1]\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- payments: double (nullable = true)\n",
      " |-- min1: double (nullable = true)\n",
      " |-- max1: double (nullable = true)\n",
      " |-- mean1: double (nullable = true)\n",
      " |-- mean6: double (nullable = true)\n",
      " |-- has_card: integer (nullable = true)\n",
      " |-- idade: integer (nullable = true)\n",
      " |-- days_between: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+----+----+-----+-----+--------+-----+------------+-----+\n",
      "|amount|duration|payments|min1|max1|mean1|mean6|has_card|idade|days_between|label|\n",
      "+------+--------+--------+----+----+-----+-----+--------+-----+------------+-----+\n",
      "|     0|       0|       0|   0|   0|    0|   98|       0|    0|           0|    0|\n",
      "+------+--------+--------+----+----+-----+-----+--------+-----+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# contar valores ausentes por coluna\n",
    "missing_counts = df_model2.select([f.sum(f.col(c).isNull().cast(\"int\")).alias(c) for c in df_model2.columns])\n",
    "\n",
    "# mostrar o resultado\n",
    "missing_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount',\n",
       " 'duration',\n",
       " 'payments',\n",
       " 'min1',\n",
       " 'max1',\n",
       " 'mean1',\n",
       " 'mean6',\n",
       " 'has_card',\n",
       " 'idade',\n",
       " 'days_between',\n",
       " 'label']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|            mean6|\n",
      "+-------+-----------------+\n",
      "|   mean|40452.95037533935|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model2.select('mean6').summary('mean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model2 = df_model2.fillna({'mean6': 40452.95})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar um VectorAssembler para transformar as variáveis explicativas em uma única coluna de vetores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o VectorAssembler para unir as colunas em uma única coluna vetorizada\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=df_model2.columns[:-1],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MinMaxScaler ou o StandardScaler para dimensionar os dados para que características como salário, idade e renda contribuam igualmente para a análise contribuam igualmente para a análise. Ao dimensionar os dados, podemos garantir que cada recurso tenha um impacto igual no desempenho do modelo, e o modelo pode fazer previsões mais precisas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conceito de normalização é implementado em Python usando MinMaxScaler e o conceito de padronização é implementado usando StandardScaler.\n",
    " MinMaxScaler dimensiona os dados para um intervalo fixo, normalmente entre 0 e 1. Por outro lado, StandardScaler redimensiona os dados para que tenham uma média de 0 e um desvio padrão de 1. Isso resulta em uma distribuição com média zero e variação de unidade. \n",
    " \n",
    " https://vitalflux.com/minmaxscaler-standardscaler-python-examples/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o StandardScaler para normalizar as variáveis\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaledFeatures\",\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o pipeline para unir as etapas de pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o Pipeline com as etapas de pré-processamento e o modelo\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(df_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = pipelineModel.transform(df_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+--------+-------+------------------+------------------+--------+-----+------------+-----+--------------------+--------------------+\n",
      "|amount|duration|payments|    min1|   max1|             mean1|             mean6|has_card|idade|days_between|label|            features|      scaledFeatures|\n",
      "+------+--------+--------+--------+-------+------------------+------------------+--------+-----+------------+-----+--------------------+--------------------+\n",
      "| 88704|      48|  1848.0| 11853.6| 9953.6|        18891.7448| 19977.38913043479|       0|   36|         670|    1|[88704.0,48.0,184...|[0.77418650865161...|\n",
      "| 88704|      48|  1848.0| 11853.6| 9953.6|        18891.7448| 19977.38913043479|       0|   39|         670|    1|[88704.0,48.0,184...|[0.77418650865161...|\n",
      "| 54396|      36|  1511.0| 18196.0| 8296.0|       27385.44375|             800.0|       0|   57|         191|    1|[54396.0,36.0,151...|[0.47475479487524...|\n",
      "|143904|      24|  5996.0|100282.7|96774.9| 49212.63181818183|        49524.9625|       0|   43|         518|    1|[143904.0,24.0,59...|[1.25595841609174...|\n",
      "|102240|      60|  1704.0| 10438.0|  900.0| 29672.60504201679|28522.888607594923|       1|   51|         607|    1|[102240.0,60.0,17...|[0.89232535899780...|\n",
      "|102240|      60|  1704.0| 10438.0|  900.0| 29672.60504201679|28522.888607594923|       0|   56|         607|    1|[102240.0,60.0,17...|[0.89232535899780...|\n",
      "|155616|      48|  3242.0| 17792.0|73261.2| 43766.96909090906|  44716.1087301587|       0|   25|         691|    1|[155616.0,48.0,32...|[1.35817784688774...|\n",
      "|155616|      48|  3242.0| 17792.0|73261.2| 43766.96909090906|  44716.1087301587|       0|   17|         691|    1|[155616.0,48.0,32...|[1.35817784688774...|\n",
      "|104712|      24|  4363.0|116809.2|97532.7| 52700.12647058825| 41260.86034482761|       0|   25|         569|    1|[104712.0,24.0,43...|[0.91390036180925...|\n",
      "|104712|      24|  4363.0|116809.2|97532.7| 52700.12647058825| 41260.86034482761|       0|   18|         569|    1|[104712.0,24.0,43...|[0.91390036180925...|\n",
      "| 56100|      60|   935.0|  1000.0| 9899.1|20549.230158730166|26157.734482758613|       0|   37|         405|    1|[56100.0,60.0,935...|[0.48962688419187...|\n",
      "|385584|      48|  8033.0|106530.1|98638.1| 71744.03055555554| 60002.15294117648|       0|   23|         502|    1|[385584.0,48.0,80...|[3.36528150649266...|\n",
      "|385584|      48|  8033.0|106530.1|98638.1| 71744.03055555554| 60002.15294117648|       0|   19|         502|    1|[385584.0,48.0,80...|[3.36528150649266...|\n",
      "| 69624|      36|  1934.0|  1100.0|74473.4|48848.771223021584| 48824.17916666667|       1|   51|         585|    1|[69624.0,36.0,193...|[0.60766100151470...|\n",
      "|100800|      24|  4200.0| -1033.8|92354.7| 28517.63666666666| 42296.14230769231|       0|   59|         469|    0|[100800.0,24.0,42...|[0.87975739619502...|\n",
      "|399120|      60|  6652.0| 12000.0|85656.2| 43888.80178571428| 48992.67142857142|       0|   54|         450|    1|[399120.0,60.0,66...|[3.48342035683885...|\n",
      "| 12540|      12|  1045.0| 14858.0|57779.4| 41539.73636363639| 38930.43495145633|       1|   57|         623|    1|[12540.0,12.0,104...|[0.10944600940759...|\n",
      "| 12540|      12|  1045.0| 14858.0|57779.4| 41539.73636363639| 38930.43495145633|       0|   61|         623|    1|[12540.0,12.0,104...|[0.10944600940759...|\n",
      "| 30276|      12|  2523.0|  1000.0| 8322.5| 31481.05952380953| 34767.38666666667|       0|   57|         388|    0|[30276.0,12.0,252...|[0.26424141792857...|\n",
      "|191088|      48|  3981.0| 12325.4|57550.1|32398.580555555556| 37199.23684210526|       0|   34|         408|    1|[191088.0,48.0,39...|[1.66776866392970...|\n",
      "+------+--------+--------+--------+-------+------------------+------------------+--------+-----+------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide o dataset em conjuntos de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainData, testData) = df_transformed.randomSplit([0.7, 0.3], seed=12345)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o modelo de Regressão Logística"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No MLLib, a Regressão Logística não possui um método de cálculo de feature importance embutido. No entanto, é possível calcular a importância das features de forma aproximada usando o método \"L1 Regularization\" (também conhecido como \"Lasso regularization\").\n",
    "\n",
    "Essa técnica de regularização penaliza os coeficientes das features que não são relevantes para a predição, forçando-os a terem valor zero. Dessa forma, as features com coeficientes não-nulos são consideradas mais importantes.\n",
    "\n",
    "Para realizar esse método no MLLib, basta usar o parâmetro \"elasticNetParam\" da função LogisticRegression, que controla a proporção de regularização L1 e L2. Ao definir o valor de \"elasticNetParam\" como 1 (que significa 100% de regularização L1), a regressão logística irá utilizar apenas L1 regularization, o que resultará em alguns coeficientes com valor zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'scaledFeatures', labelCol = 'label', elasticNetParam=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define os parâmetros a serem testados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#paramGrid = ParamGridBuilder() \\\n",
    "#    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "#    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "#    .build()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o avaliador para a validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol='label',\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo a validação cruzada com 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \n",
    "#crossval = CrossValidator(\n",
    "#    estimator=lr,\n",
    "#    estimatorParamMaps=paramGrid,\n",
    "#    evaluator=evaluator,\n",
    "#    numFolds=5\n",
    "#)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajusta o modelo com a validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#modelo = crossval.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "modelo = lr.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = modelo.coefficients.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.44140367,  0.19925389, -0.19457131,  0.46574029, -0.36123858,\n",
       "        0.67418071, -0.28387973,  0.39524176, -0.11496371,  0.31963519])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtém as features mais importantes (aquelas com coeficiente não nulo)\n",
    "important_features = [i for i, coef in enumerate(coefficients) if coef != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = list(zip(assembler.getInputCols(), modelo.coefficients.toArray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amount', -0.44140366598496533),\n",
       " ('duration', 0.1992538902862431),\n",
       " ('payments', -0.1945713091696159),\n",
       " ('min1', 0.46574029122558336),\n",
       " ('max1', -0.36123857645703733),\n",
       " ('mean1', 0.6741807087636067),\n",
       " ('mean6', -0.2838797275156386),\n",
       " ('has_card', 0.3952417574842975),\n",
       " ('idade', -0.11496370821418399),\n",
       " ('days_between', 0.31963519439202237)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on training data = 0.749644\n"
     ]
    }
   ],
   "source": [
    "# assuming `model` is your trained LogisticRegressionModel object, and `trainData` is your training data DataFrame\n",
    "predictions_train = modelo.transform(trainData)\n",
    "auc_train = evaluator.evaluate(predictions_train)\n",
    "print(\"AUC on training data = %g\" % auc_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avalia o modelo com os dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on training data = 0.737362\n"
     ]
    }
   ],
   "source": [
    "# assuming `model` is your trained LogisticRegressionModel object, and `trainData` is your training data DataFrame\n",
    "predictions_teste = modelo.transform(testData)\n",
    "auc_test = evaluator.evaluate(predictions_teste)\n",
    "print(\"AUC on training data = %g\" % auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- payments: double (nullable = true)\n",
      " |-- min1: double (nullable = true)\n",
      " |-- max1: double (nullable = true)\n",
      " |-- mean1: double (nullable = true)\n",
      " |-- mean6: double (nullable = false)\n",
      " |-- has_card: integer (nullable = true)\n",
      " |-- idade: integer (nullable = true)\n",
      " |-- days_between: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_teste.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|probability                              |\n",
      "+-----------------------------------------+\n",
      "|[0.06381757357592582,0.9361824264240741] |\n",
      "|[0.05330746782871899,0.946692532171281]  |\n",
      "|[0.0376993154477843,0.9623006845522157]  |\n",
      "|[0.0483672752805157,0.9516327247194843]  |\n",
      "|[0.06129740607371455,0.9387025939262854] |\n",
      "|[0.06145562059614347,0.9385443794038565] |\n",
      "|[0.06764570640926422,0.9323542935907357] |\n",
      "|[0.07714130316857318,0.9228586968314269] |\n",
      "|[0.07971605815726494,0.9202839418427351] |\n",
      "|[0.08715836878834726,0.9128416312116528] |\n",
      "|[0.05752114435269332,0.9424788556473066] |\n",
      "|[0.0656127390810451,0.934387260918955]   |\n",
      "|[0.07336429977526526,0.9266357002247346] |\n",
      "|[0.030827146560480762,0.9691728534395193]|\n",
      "|[0.06953499812460485,0.9304650018753952] |\n",
      "|[0.04807060980393369,0.9519293901960664] |\n",
      "|[0.020967535206314333,0.9790324647936857]|\n",
      "|[0.05312503023746343,0.9468749697625365] |\n",
      "|[0.04094937631443946,0.9590506236855606] |\n",
      "|[0.10532273878575012,0.8946772612142498] |\n",
      "+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_teste.select('probability').show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando as métricas do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "BinaryClassificationEvaluator_3b9cdd8ca4a3 parameter metricName given invalid value precisionByLabel.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-bb90eb56aea4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_teste\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_teste\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"precisionByLabel\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_teste\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"recallByLabel\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy = %g\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Precision = %g\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, extra)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[0mthat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_empty_java_param_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mthat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mthat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_transfer_params_to_java\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                 \u001b[0mpair\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_java_param_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasDefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_make_java_param_pair\u001b[1;34m(self, param, value)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mjava_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mjava_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mjava_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: BinaryClassificationEvaluator_3b9cdd8ca4a3 parameter metricName given invalid value precisionByLabel."
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(predictions_teste, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions_teste, {evaluator.metricName: \"precisionByLabel\"})\n",
    "recall = evaluator.evaluate(predictions_teste, {evaluator.metricName: \"recallByLabel\"})\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Precision = %g\" % precision)\n",
    "print(\"Recall = %g\" % recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
